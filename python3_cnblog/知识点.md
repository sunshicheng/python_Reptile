学习链接地址: https://www.cnblogs.com/liuxiaosong/category/1384979.html

### day01: 使用urllib模块

1. urllib几个模块

   - urllib.reuqest 请求模块
   - urllib.parse 分析模块
   - urllib.error 错误处理模块
   - urllib.robotparser robots文本协议识别 用的比较少

2. 几个常用的方法

   - urllib.request,urlopen()

     ```python
     import urllib.request
     
     response = urllib.request.urlopen("https://www.python.org")
     
     print(response.read().decode('utf-8'))
     ```

   - 获取网站的一些信息type()方法 read()方法 status 方法 getheaders（） getheader('Server')

     ```python
     import urllib.request
     
     response = urllib.request.urlopen("https://www.python.org")
     
     print(type(response))
     print(response.read())
     print(response.status)
     print(response.getheaders())
     print(response.getheader('Server'))
     ```

3. 几个常用的参数

   - data 内容

     ```python
     import urllib.parse
     import urllib.request
     import ssl
     ssl._create_default_https_context = ssl._create_unverified_context
     
     data = bytes(urllib.parse.urlencode({'name':'hello','pass':'123'}),encoding='utf-8')
     response = urllib.request.urlopen('http://www.iqianyue.com/mypost',data=data)
     print(response.read())
     ```

   - timeout 超时设置

     ```python
     import urllib.request
     response = urllib.request.urlopen('http://www.python.org',timeout=1)
     print(response.text)
     
     import socket
     import urllib.request
     import urllib.error
     
     try:
         response = urllib.request.urlopen('http://httpbin.org/get',timeout=1)
     except urllib.error.URLError as e:
         if isinstance(e.reason,socket.timeout):
             print('TIME OUT')
     ```

   - urllib.request模块的使用

   - quote()将内容转换为url编码

   - unquote() 解码

4. 一个实例，爬去百度贴吧的实例，将百度贴吧的相关url拼接，然后将返回的页面保存到本地

   ```python
   import urllib
   import urllib.request
   import urllib.parse
   
   
   # 百度贴吧爬虫接口 组合url地址 起始页和终止页
   def tiebaSpider(url, beginPage, endPage):
       """
       作用：负责处理 url 分配每一个url去发送请求
       :param url: 处理第一个url
       :param beginPage: 爬虫起始页
       :param endPage: 爬虫终止页
       :return: null
       """
       for Page in range(beginPage, endPage + 1):
           pn = (Page - 1) * 50
           filename = "第" + str(Page) + "页.html"
           # 组合url 发送请求
           fullurl = url + "&pn=" + str(pn)
           # print fullurl
           # 调用loadPage（）函数发送请求获取HTML页面
           html = loadPage(fullurl, filename)
           # 调用writePage()函数 将服务器响应文件保存到本地磁盘
           writeFile(html, filename)
   
   
   def loadPage(url, filename):
       """
       作用：根据url发送请求 获取服务器响应数据
       :param url: 请求地址
       :param filename: 文件名
       :return:服务器响应文件
       """
       print("正在下载" + filename)
       headers = {
           "User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36"}
       request = urllib.request.Request(url, headers=headers)
       response = urllib.request.urlopen(request)
       return response.read()
   
   
   def writeFile(html, filename):
       """
       作用：保存服务器文件到本地磁盘
       :param html: 服务器文件
       :param filename:本地磁盘文件名
       :return:null
       """
       print("正在存储" + filename)
       with open(filename, "wb+") as f:
           f.write(html)
       print("-" * 20)
   
   
   if __name__ == "__main__":
       kw = input("请输入要爬取的贴吧名")
       # 输入起始页和终止页 str转化为int类型
       beginPage = int(input("请输入爬取的起始页"))
       endPage = int(input("请输入爬取的终止页"))
       url = "http://tieba.baidu.com/f?"
       key = urllib.parse.urlencode({"kw": kw})
       # 组合后的url示例 http://tieba.baidu.com/f?kw=lol
       url = url + key
       tiebaSpider(url, beginPage, endPage)
   ```

5. 异常处理
    ```python
       当使用urlopen 出现ssl证书错误的时候，可以使用下面这个方法进行处理
       import ssl
       ssl._create_default_https_context = ssl._create_unverified_context
    ```

6. python 字符串基础

   ```
   str为字符串
   str.isalnum() 所有字符都是数字或者字母
   str.isalpha() 所有字符都是字母
   str.isdigit() 所有字符都是数字
   str.islower() 所有字符都是小写
   str.isupper() 所有字符都是大写
   str.istitle() 所有单词都是首字母大写，像标题
   str.isspace() 所有字符都是空白字符、\t、\n、\r
   ```

   

### day 02 :使用requests模块

1. 安装 pip install 